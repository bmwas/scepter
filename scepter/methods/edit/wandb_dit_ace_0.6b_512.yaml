ENV:
  BACKEND: nccl
  SEED: 2024
#
SOLVER:
  NAME: ACESolver
  RESUME_FROM:
  LOAD_MODEL_ONLY: True
  USE_FSDP: False
  SHARDING_STRATEGY:
  USE_AMP: True
  DTYPE: float16
  CHANNELS_LAST: True
  MAX_STEPS: 5000
  MAX_EPOCHS: -1
  NUM_FOLDS: 10
  ACCU_STEP: 1
  EVAL_INTERVAL: 5
  RESCALE_LR: False
  #
  WORK_DIR: ./cache/save_data/ace_0.6b_512
  LOG_FILE: std_log.txt
  #
  FILE_SYSTEM:
    - NAME: "HuggingfaceFs"
      TEMP_DIR: ./cache/cache_data
    - NAME: "LocalFs"
      TEMP_DIR: ./cache/cache_data
    - NAME: "ModelscopeFs"
      TEMP_DIR: ./cache/cache_data

  #
  MODEL:
    NAME: LatentDiffusionACE
    PRETRAINED_MODEL:
    IGNORE_KEYS: [ ]
    SCALE_FACTOR: 0.18215
    SIZE_FACTOR: 8
    DECODER_BIAS: 0.5
    DEFAULT_N_PROMPT:
    USE_EMA: True
    EMA_DECAY: 0.99
    EVAL_EMA: False
    USE_SWA: True  # Enable Stochastic Weight Averaging for extreme stability
    SWA_START: 300  # Increased from 100 to delay averaging
    SWA_FREQ: 10    # Reduced frequency for less smoothing early on
    TEXT_IDENTIFIER: [ '{image}', '{image1}', '{image2}', '{image3}', '{image4}', '{image5}', '{image6}', '{image7}', '{image8}', '{image9}' ]
    USE_TEXT_POS_EMBEDDINGS: True
    TUNER:
      NAME: SwiftLoRA
      R: 256  # Reverting to original high value
      LORA_ALPHA: 512  # Original high alpha for strong adaptation
      LORA_DROPOUT: 0.00  # Original no dropout
      CROSS_ATTENTION_SCALE: 3.0  # Original high value
      BIAS: "all"
      TARGET_MODULES:
        - "attn.q"
        - "attn.k"
        - "attn.v"
        - "attn.o"
        - "cross_attn.q"
        - "cross_attn.k"
        - "cross_attn.v"
        - "cross_attn.o"
        - "ff.net.0"
        - "ff.net.2"
    #
    DIFFUSION:
      NAME: BaseDiffusion
      PREDICTION_TYPE: eps
      MIN_SNR_GAMMA: 3.0  # Reduced from 5.0 to prevent excessive gradient suppression
      NOISE_SCHEDULER:
        NAME: LinearScheduler
        NUM_TIMESTEPS: 1000
        BETA_MIN: 0.0001
        BETA_MAX: 0.02
    #
    DIFFUSION_MODEL:
      NAME: ACE
      PRETRAINED_MODEL: hf://scepter-studio/ACE-0.6B-512px@models/dit/ace_0.6b_512px.pth
      IGNORE_KEYS: [ ]
      PATCH_SIZE: 2
      IN_CHANNELS: 4
      HIDDEN_SIZE: 1152
      DEPTH: 28
      NUM_HEADS: 16
      MLP_RATIO: 4.0
      PRED_SIGMA: True
      DROP_PATH: 0.0
      WINDOW_DIZE: 0
      Y_CHANNELS: 4096
      MAX_SEQ_LEN: 1024
      QK_NORM: True
      USE_GRAD_CHECKPOINT: True
      ATTENTION_BACKEND: flash_attn
    #
    FIRST_STAGE_MODEL:
      NAME: AutoencoderKL
      EMBED_DIM: 4
      PRETRAINED_MODEL: hf://scepter-studio/ACE-0.6B-512px@models/vae/vae.bin
      IGNORE_KEYS: []
      #
      ENCODER:
        NAME: Encoder
        CH: 128
        OUT_CH: 3
        NUM_RES_BLOCKS: 2
        IN_CHANNELS: 3
        ATTN_RESOLUTIONS: [ ]
        CH_MULT: [ 1, 2, 4, 4 ]
        Z_CHANNELS: 4
        DOUBLE_Z: True
        DROPOUT: 0.0
        RESAMP_WITH_CONV: True
      #
      DECODER:
        NAME: Decoder
        CH: 128
        OUT_CH: 3
        NUM_RES_BLOCKS: 2
        IN_CHANNELS: 3
        ATTN_RESOLUTIONS: [ ]
        CH_MULT: [ 1, 2, 4, 4 ]
        Z_CHANNELS: 4
        DROPOUT: 0.0
        RESAMP_WITH_CONV: True
        GIVE_PRE_END: False
        TANH_OUT: False
    #
    COND_STAGE_MODEL:
      NAME: T5EmbedderHF
      PRETRAINED_MODEL: hf://scepter-studio/ACE-0.6B-512px@models/text_encoder/t5-v1_1-xxl/
      TOKENIZER_PATH: hf://scepter-studio/ACE-0.6B-512px@models/tokenizer/t5-v1_1-xxl
      LENGTH: 120
      T5_DTYPE: bfloat16
      ADDED_IDENTIFIER: [ '{image}', '{caption}', '{mask}', '{ref_image}', '{image1}', '{image2}', '{image3}', '{image4}', '{image5}', '{image6}', '{image7}', '{image8}', '{image9}' ]
      CLEAN: whitespace
      USE_GRAD: False
    LOSS:
      NAME: ReconstructLoss
      LOSS_TYPE: l2  # Original was l2
  #
  SAMPLE_ARGS:
    SAMPLER: ddim
    SAMPLE_STEPS: 20  # Increased from 20
    GUIDE_SCALE: 4.5  # Higher guidance scale for stronger conditioning
    GUIDE_RESCALE: 0.5  # Increased from 0.5
  #
  OPTIMIZER:
    NAME: AdamW
    LEARNING_RATE: 1e-7
    EPS: 1e-10
    WEIGHT_DECAY: 5e-4
  #
  TRAIN_DATA:
    NAME: CSVInRAMDataset
    MODE: train
    CSV_PATH: ./cache/datasets/hed_test/larger_hed_pair/training.csv
    IMAGE_ROOT_DIR: ./cache/datasets/hed_test
    BATCH_SIZE: 20
    NUM_WORKERS: 1
    SAMPLER:
      NAME: LoopSampler
      SHUFFLE: true
    DROP_LAST: false
  #
  VAL_DATA:
    NAME: CSVInRAMDataset
    MODE: validation
    CSV_PATH: ./cache/datasets/hed_test/larger_hed_pair/validation.csv
    IMAGE_ROOT_DIR: ./cache/datasets/hed_test
    BATCH_SIZE: 1
    NUM_WORKERS: 1
    DROP_LAST: false
    INPUT_FORMAT:
      PROMPT_AS_NESTED_LIST: true
      EMPTY_SRC_IMAGE_LIST: true
  #
  TRAIN_HOOKS:
    - {NAME: WandbConfigLogHook, PRIORITY: 1}
    - {NAME: CopyConfigHook, PRIORITY: 10}
    # ---------------- core training ----------------
    - {NAME: BackwardHook,   PRIORITY: 0}
    - {NAME: LogHook,        LOG_INTERVAL: 50, PRIORITY: 100}

    # ---------------- checkpoints & metrics ----------------
    - {NAME: CheckpointHook, INTERVAL: 500, PRIORITY: 300,
      MAX_TO_KEEP: 1,
      PUSH_TO_HUB: false,
      HUB_MODEL_ID: "Benson/ace-model-0.6b-512",
      HUB_PRIVATE: false,
      SAVE_LORA: true,
      HUB_TOKEN: "${env:HUGGINGFACE_TOKEN}"}
      
    # ---------------- Complete model with all components ----------------
    - {NAME: FinalModelHFHook, PRIORITY: 1200,
      OUTPUT_DIR: "FINAL_MODEL_HF",
      SAVE_ON_STEPS: [200],  # Save at these specific steps
      MODEL_COMPONENTS: ["dit", "text_encoder", "tokenizer", "vae"],
      PUSH_TO_HUB: true,
      HUB_MODEL_ID: "Benson/ace-model-0.6b-512-complete",
      HUB_PRIVATE: false,
      HUB_TOKEN: "${env:HUGGINGFACE_TOKEN}"}
      
    # ---------------- LoRA model saving hook using standard hook ----------------
    - {NAME: FinalModelHFHook, PRIORITY: 1100,
      OUTPUT_DIR: "FINAL_LORA_HF",
      SAVE_ON_STEPS: [200],
      SAVE_LORA_ONLY: true,
      MODEL_COMPONENTS: ["lora_adapters"],  # Only save LoRA adapter weights
      PUSH_TO_HUB: true,
      HUB_MODEL_ID: "Benson/ace-model-0.6b-512-lora-final",
      HUB_PRIVATE: false,
      HUB_TOKEN: "${env:HUGGINGFACE_TOKEN}"}
      
    # ---------------- LoRA visualization hook ----------------
    - {NAME: LoRAWandbVizHook, PRIORITY: 500,
      VIZ_INTERVAL: 15,         # Generate images every 25 steps
      VIZ_START: 2,             # Start at step 5
      NUM_VAL_SAMPLES: 5,       # Number of validation samples to use
      FIXED_SAMPLE_INDICES: [0, 1, 2, 3, 4],  # Use first 5 samples consistently
      NUM_INFERENCE_STEPS: 30,  # Increased from 30 for cleaner edges
      GUIDANCE_SCALE: 12.0,     # Higher for stronger conditioning
      CONTRAST_BOOST: 1.5,      # New parameter to enhance edge visibility
      IMAGE_SIZE: 512,
      LOG_PROMPTS: true,        # Log the prompts used for visualization
      }
      
    # ---------------- File tracking and documentation ----------------
    - {NAME: WandbFileTrackerHook, PRIORITY: 400,
      TRACK_INTERVAL: 50,     # More frequent checks (was 300)
      TRACK_ON_START: true,   # Added to ensure tracking at beginning of training
      WATCHED_DIRECTORIES: [
        "./cache/save_data",   # Main save directory
        "./cache/datasets/hed_test/",  # Updated CSV directory
        "./scepter/methods/edit/",
      ],
      SPECIFIC_FILES: [
        "./scepter/methods/edit/wandb_dit_ace_0.6b_512.yaml"  # Using absolute path
      ],
      FILE_EXTENSIONS: [
        ".csv", ".yaml", ".yml", ".json", ".txt",  # Data and config files
        ".pth", ".bin", ".pt",".csv",                    # Model weights
        ".png", ".jpg", ".jpeg"                   # Generated images
      ],
      CREATE_RESULTS_ARTIFACT: true,
      ARTIFACT_NAME: "training_files",
      ARTIFACT_TYPE: "dataset",
      ARTIFACT_DESCRIPTION: "Training configuration and dataset files"}
      
    # ---------------- images / probe data -----------------
    - {NAME: ProbeDataHook,       PROB_INTERVAL: 100}                        # keeps local saves
